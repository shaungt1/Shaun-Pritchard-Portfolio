# -*- coding: utf-8 -*-
"""SPritchard_10252022_CAP6686_MIMICIII_DATA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14aqwY6E19HkrBd7u-N5wYpLOvk-N2mk_

##**Bonus Assignment**
- Shaun Pritchard
- CAP -6683 AI In health and Medicine
- 10/25/2022 

SPritchard_10252022_CAP6686_MIMICIII_DATA.ipynb

# Predicitng Readmission MIMIC III data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
# Text Processing
import re

admissions = pd.read_csv("./Data/ADMISSIONS.csv", index_col = None)
patients = pd.read_csv("./Data/PATIENTS.csv", index_col = None)

"""# Convert all the date columns

"""

# Let us sort the dataframe on admittime and subjectid to see how a post admission journey looks like
admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME'])
admissions.reset_index(drop = True, inplace = True)

# Since we already have the patient's admit time and DOB from patients table let us calculate a patient's age
patient_age = {row[1]: row[2] for row in patients[['SUBJECT_ID','DOB']].itertuples()}
admissions["AGE"] = [int((adm_time.date() - patient_age[subj_id].date()).days/365) 
                     for adm_time, subj_id in zip(admissions["ADMITTIME"], admissions["SUBJECT_ID"])]

age_plot = admissions.AGE.hist()
age_plot.set_xlabel('Age of Patients')
age_plot.set_ylabel('Count of Patients')

# The above histogram shows us that that more or less the age is spread upto 100, but there are a lot of patients with age of 300. 
# Don't be confused this is just because for patients with age >89 are replaced with 300 in the MIMIC III dataset due to HIPPA

# Let us randomly spread these patients within age range of 90 to 100
admissions.loc[admissions.AGE >= 300,"AGE"] = random.choices(list(range(90,100)),k = sum(admissions.AGE >= 300))

# We will also remove all the young patients as chance of readmission is quite low due to rare chance of comorbidity and good general health
# This will also help us to remove imablance between positive and negative class
admissions = admissions[admissions.AGE >18]

def normalize_ethnicity(x):
    """
    Helper Function to Normalize Ethnicity into "WHITE", "HISPANIC", "ASIAN", "BLACK" and "OTHERS"
    """
    if "WHITE" in x:
        return "WHITE"
    elif "HISPANIC" in x:
        return "HISPANIC"
    elif "ASIAN" in x:
        return "ASIAN"
    elif "BLACK" in x:
        "BLACK"
    else:
        return "OTHERS"

admissions.ETHNICITY.value_counts().head(10).sort_values().plot(kind = "barh")

# Next, Let us normalize Ethnicity for the same reason to get substantial support for different categories and we don't end up having
# a very sparse data
admissions.ETHNICITY.value_counts()
admissions.ETHNICITY = admissions.ETHNICITY.apply(lambda x: normalize_ethnicity(x) if pd.notnull(x) else x)

def normalize_discharge(x):
    """
    Helper Function to Normalize Discharge Location into "HOME", "MEDICAL_FACILITY", and "OTHERS"
    """
    if "HOME" in x:
        return "HOME"
    elif len(re.findall("OTHER|DEAD",x)) > 0:
        return "OTHER"
    else:
        return "MEDICAL_FACILITY"

# Let us club Discharge location into three categories Medical Facility, Home and Others. This will help us maintain a significant count
# For each category 
# Let us Quickly see unique Discharge Locations
admissions.DISCHARGE_LOCATION.value_counts()
admissions.DISCHARGE_LOCATION = admissions.DISCHARGE_LOCATION.apply(lambda x: normalize_discharge(x) if pd.notnull(x) else x)

# Discharge Duration
admissions["DISCHARGE_DURATION"] = (admissions["DISCHTIME"] - admissions["ADMITTIME"]).dt.total_seconds()/(24*60*60)

# Calculating Days Until Next Admission
# Step 1:- Add the next Admit Time
admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME']) #make sure the admittime is sorted before the shift operation
admissions['NEXT_ADMITTIME'] = admissions.groupby('SUBJECT_ID').ADMITTIME.shift(-1)

# Step 2:- Subtract Discharge Time from Next Admit Time
admissions['DAYS_NEXT_ADMIT']=  (admissions.NEXT_ADMITTIME - admissions.DISCHTIME).dt.total_seconds()/(24*60*60)

admissions["IS_READMISSION"] = admissions.DAYS_NEXT_ADMIT.apply(lambda x: 0 if pd.isnull(x) else (0 if x >30 else 1))

# Lastly we will remove those any death related admission event.
admissions = admissions[admissions.HOSPITAL_EXPIRE_FLAG == 0].reset_index(drop = True)

# 1) We need only Unplanned medical care and our patient cohort should not represent new borns hence we will filter out
# "ELECTIVE" and "NEWBORN"
admissions.ADMISSION_TYPE.value_counts()
admissions = admissions[~admissions.ADMISSION_TYPE.isin(["ELECTIVE", "NEWBORN"])].reset_index(drop = True)

admissions = admissions[["SUBJECT_ID", "HADM_ID", "AGE", "ADMISSION_TYPE","DISCHARGE_DURATION","DISCHARGE_LOCATION","INSURANCE","ETHNICITY","IS_READMISSION","ADMITTIME"]]

admissions = pd.merge(admissions, patients[["SUBJECT_ID","GENDER"]], how="left", on = "SUBJECT_ID")

icustays = pd.read_csv("./Data/ICUSTAYS.csv", index_col = None)
transfers = pd.read_csv("./Data/TRANSFERS.csv", index_col = None)

# Convert all the date columns
icustays.INTIME = pd.to_datetime(icustays.INTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')
icustays.OUTTIME = pd.to_datetime(icustays.OUTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')

transfers.dropna(subset=["ICUSTAY_ID"], inplace = True)
transfers.ICUSTAY_ID = transfers.ICUSTAY_ID.astype(int)

transfers.EVENTTYPE.value_counts()

transfers_num = transfers.groupby(["SUBJECT_ID","HADM_ID","ICUSTAY_ID"])['EVENTTYPE'].apply(lambda x : sum(x=="transfer")).reset_index()

transfers_num.columns = ["SUBJECT_ID","HADM_ID","ICUSTAY_ID", "NUM_TRANSFERS"]

# Updating ICU Data with number of transfer a patient undergoes once admitted
# Higher number of transfers translates to higher serious risk at the patient's end
# But this is just number of transfers happened within 24hrs of first ICU admission
icustays = pd.merge(icustays, transfers_num, on=["SUBJECT_ID","HADM_ID","ICUSTAY_ID"], how="left")

# Making sure that if a key is not found then number of transfers for that key automatically becomes 0
icustays.NUM_TRANSFERS.fillna(0, inplace = True)

# ICU Transfers within 24hrs for a unique hospital admission
icustays_transfers_num = icustays.groupby(["SUBJECT_ID","HADM_ID"])["NUM_TRANSFERS"].sum().reset_index()

# ICU Transfers across days for a unique hospital admission
icustays_num = icustays.groupby(["SUBJECT_ID","HADM_ID"])["ICUSTAY_ID"].nunique().reset_index()
icustays_num.columns = ["SUBJECT_ID","HADM_ID","ICU_TRANSFERS"]

# Average Length of stay in ICU for a patient
icustays_avg_los = icustays.groupby(["SUBJECT_ID","HADM_ID"])["LOS"].mean().reset_index()

# We should also get the first care unit for the admission
icustays = icustays.sort_values(['SUBJECT_ID','HADM_ID','INTIME'])
icustays_firstcare = icustays.groupby(['SUBJECT_ID','HADM_ID'])['FIRST_CAREUNIT'].nth(0).reset_index()

import functools
_dfs = [icustays_num, icustays_avg_los, icustays_transfers_num, icustays_firstcare]
icustays_final = functools.reduce(lambda left,right: pd.merge(left,right,on=["SUBJECT_ID","HADM_ID"], how="inner"), _dfs)

icustays_transfers_num.reset_index()

icustays_final["TOTAL_TRANSFERS"] = icustays_final["ICU_TRANSFERS"] + icustays_final["NUM_TRANSFERS"]

# WE will drop any Neo (New Born) related ICU visits, https://mimic.physionet.org/mimictables/transfers/
icustays_final = icustays_final[~icustays_final.FIRST_CAREUNIT.isin(["NICU","NWARD"])].reset_index(drop = True).drop(["NUM_TRANSFERS","ICU_TRANSFERS"], axis = 1)

# It's time to move to patients clincal and lab data
# These are large files and hence we need a smarter a way to ingest them into our space. 
# Also since we don't need full data we will iterate using chunking a in-built feature of pandas library

dictionary_itemid = pd.read_csv("./Data/D_ITEMS.csv", index_col = None)
dictionary_itemid.dropna(subset=["LABEL"], inplace = True)

# We only need those ITEM IDs which links to chart events
dictionary_itemid = dictionary_itemid[dictionary_itemid.LINKSTO.isin(["chartevents"])]

# To get the item-ids, follow these steps
# Step 1:- Make a combination of words you expect to show up as description
# Step 2:- Use your Domain Knowledge to filter down the ITEMIDs
dictionary_itemid[[ True if ("sys" in x.lower() and len(re.findall("bp|blood pressure|blood",x.lower())) > 0) else False for x in dictionary_itemid.LABEL]]
sys_bp_itemids = [51, 442, 6701, 220050, 220179]

dictionary_itemid[[ True if ("dia" in x.lower() and len(re.findall("bp|blood pressure|blood",x.lower())) > 0) else False for x in dictionary_itemid.LABEL]]
dia_bp_itemids = [8368, 8440, 8555, 220051, 220180]

dictionary_itemid[[ True if ("resp" in x.lower() and len(re.findall("rate",x.lower())) > 0) else False for x in dictionary_itemid.LABEL]]
respr_itemids = [615, 618, 3603, 224690, 220210]

dictionary_itemid[[ True if ("glucose" in x.lower()) else False for x in dictionary_itemid.LABEL]]
glucose_itemids = [1455, 1310, 807, 811, 3744, 3745, 1529, 2338, 225664, 220621, 226537]

# Similarly
heartrate_itemids = [211, 220045]
temp_itemids = [676, 678, 223761, 223762]

# Using the same idea we will get a short list of itemids for the list of lab events we are using
dictionary_labitemid = pd.read_csv("./Data/D_LABITEMS.csv", index_col = None)

dictionary_labitemid.dropna(subset=["LABEL","FLUID"], inplace = True)
dictionary_labitemid["DEFINITION"] = dictionary_labitemid["LABEL"] + " " +dictionary_labitemid["FLUID"]

hadm_filter = icustays_final.HADM_ID.tolist()
total_itemids = sys_bp_itemids+dia_bp_itemids+respr_itemids+glucose_itemids+temp_itemids+heartrate_itemids
total_labitems = [51265, 51221, 50862, 50983, 50971, 50893]
n_rows = 100000

# create the iterator
labevents_iterator = pd.read_csv(
    "./Data/LABEVENTS.csv",
    iterator=True,
    chunksize=n_rows)

# concatenate according to a filter to get our labevents data
labevents = pd.concat(
    [labevent_chunk[np.logical_and(labevent_chunk['HADM_ID'].isin(hadm_filter),
                                  labevent_chunk['ITEMID'].isin(total_labitems))]if str(labevent_chunk.HADM_ID.dtype) == 'int64'
                   else labevent_chunk[np.logical_and(labevent_chunk['HADM_ID'].isin([float(x) for x in hadm_filter]),
                                                      labevent_chunk['ITEMID'].isin(total_labitems))]
    for labevent_chunk in labevents_iterator])

# create the iterator
chartevents_iterator = pd.read_csv(
    "./Data/CHARTEVENTS.csv",
    iterator=True,
    chunksize=n_rows,
    usecols = ["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "ITEMID", "VALUE", "VALUENUM", "VALUEUOM"])

# concatenate according to a filter to get our labevents data
chartevents = pd.concat(
    [chartevent_chunk[np.logical_and(chartevent_chunk['HADM_ID'].isin(hadm_filter),
                                    chartevent_chunk['ITEMID'].isin(total_itemids))] if str(chartevent_chunk.HADM_ID.dtype) == 'int64'
                   else chartevent_chunk[np.logical_and(chartevent_chunk['HADM_ID'].isin([float(x) for x in hadm_filter]),
                                                       chartevent_chunk['ITEMID'].isin(total_itemids))]
    for chartevent_chunk in chartevents_iterator])

chartevents.dropna(axis = 0, subset = ["VALUENUM"], inplace = True)

chartevents.drop('VALUE', axis = 1, inplace = True)

# Since the data is collected from two different systems let us check for units for each of our patients clinical data
print("Systolic BP :- ",chartevents[chartevents.ITEMID.isin(sys_bp_itemids)].VALUEUOM.unique())
print("Diastolic BP :- ",chartevents[chartevents.ITEMID.isin(dia_bp_itemids)].VALUEUOM.unique())
print("Respiratory Rate :- ",chartevents[chartevents.ITEMID.isin(respr_itemids)].VALUEUOM.unique())
print("Glucose Levels :- ",chartevents[chartevents.ITEMID.isin(glucose_itemids)].VALUEUOM.unique())
print("Heart Rate :- ",chartevents[chartevents.ITEMID.isin(heartrate_itemids)].VALUEUOM.unique())
print("Temperature :- ",chartevents[chartevents.ITEMID.isin(temp_itemids)].VALUEUOM.unique())

# Let us Replace ItemIds by their respective Chart Event Names to aid readability
mapping = {"Systolic_BP":sys_bp_itemids,
          "Diastolic_BP":dia_bp_itemids,
          "Resp_Rate":respr_itemids,
          "Glucose":glucose_itemids,
          "Heart_Rate":heartrate_itemids,
          "Temperature":temp_itemids}

item_id_map = {item_id: k for k,v in mapping.items() for item_id in v}

chartevents["ITEMID"] = chartevents["ITEMID"].replace(item_id_map)

# insp/min is same as BPM and hence there is no conversion required here
# and we won't impute for na for Glucose as the value are in the same range as when the unit is present
# We need to convert Farenheit to celsius.
chartevents.isnull().sum()

cond1 = np.logical_and(np.logical_or(chartevents["VALUEUOM"] == "?F", chartevents["VALUEUOM"] == "Deg. F"),
               pd.notnull(chartevents["VALUEUOM"])).tolist()
cond2 = np.logical_or(chartevents["VALUEUOM"] != "?F", chartevents["VALUEUOM"] != "Deg. F").tolist()
condval1 = ((chartevents["VALUENUM"]-32)*5/9).tolist()
condval2= chartevents["VALUENUM"].tolist()
chartevents["VALUENUM"] = np.select([cond1, cond2], [condval1,condval2])

charts = chartevents.pivot_table(index=['SUBJECT_ID', 'HADM_ID'], 
                                   columns='ITEMID', values='VALUENUM', 
                                   aggfunc=[np.mean, np.std]).reset_index()

charts.columns = charts.columns.get_level_values(0)+'_'+charts.columns.get_level_values(1)

charts.isnull().sum()

charts = charts.groupby(['SUBJECT_ID_']).apply(lambda x: x.bfill())

charts.isnull().sum()

charts.columns = [x[:-1] for x in charts.columns[:2]] + list(charts.columns[2:])

labevents.dropna(axis = 0, subset = ["HADM_ID"], inplace = True)
labevents.HADM_ID = labevents.HADM_ID.astype(int)

labevents_label = dictionary_labitemid[dictionary_labitemid.ITEMID.isin(total_labitems)]
item_id_map = dict(zip(labevents_label.ITEMID,labevents_label.LABEL))

labevents["ITEMID"] = labevents["ITEMID"].replace(item_id_map)

# No need to normalize units of labevents
labevents.groupby(["ITEMID"])['VALUEUOM'].apply(lambda x: set(x))

labs = labevents.pivot_table(index=['SUBJECT_ID', 'HADM_ID'], 
                                   columns='ITEMID', values='VALUENUM', 
                                   aggfunc=[np.mean, np.std]).reset_index()
labs.columns = labs.columns.get_level_values(0)+'_'+labs.columns.get_level_values(1)

labs.isnull().sum()

labs = labs.groupby(['SUBJECT_ID_']).apply(lambda x: x.bfill())

labs.isnull().sum()

labs.columns = [x[:-1] for x in labs.columns[:2]] + list(labs.columns[2:])

"""## Commorbidities

-https://github.com/MIT-LCP/mimic-code/blob/master/concepts/comorbidity/elixhauser_quan.sql

-https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381763/
"""

diagnosis_icd = pd.read_csv("./Data/DIAGNOSES_ICD.csv", index_col = None)

mapping = {'congestive_heart_failure':['39891','40201','40211','40291','40401','40403','40411','40413','40491',
                                     '40493','4254','4255','4257','4258','4259',
                                    '428'],
       'cardiac_arrhythmias':['42613','42610','42612','99601','99604','4260','4267',
            '4269','4270','4271','4272','4273','4274','4276','4278','4279','7850','V450','V533'],
       'valvular_disease':['0932','7463','7464','7465','7466','V422','V433',
                            '394','395','396','397','424'],
       'pulmonary_circulation_disorder':['4150','4151','4170','4178','4179',
                                          '416'],
       'peripheral_vascular_disorder':['0930','4373','4431','4432','4438','4439','4471','5571','5579','V434',
                                        '440','441'],
       'hypertension':['401','402','403','404','405'],
       'paralysis':['3341','3440','3441','3442','3443','3444','3445','3446','3449',
                                     '342','343'],
       'other_neurological':['33392', 
                              '3319','3320','3321','3334','3335','3362','3481','3483','7803','7843', 
                             '334','335','340','341','345'],
       'chronic_pulmonary_disease':['4168','4169','5064','5081','5088',
                                    '490','491','492','493','494','495','496','500','501','502','503','504','505'],
       'diabetes_w_complications':['2504','2505','2506','2507','2508','2509'],
       'hypothyroidism':['2409','2461','2468', '243','244'],
       'renal_failure':['40301','40311','40391','40402','40403','40412','40413','40492','40493',
                        '5880','V420','V451',
                        '585','586','V56'],
       'liver_disease':['07022','07023','07032','07033','07044','07054',
                        '0706','0709','4560','4561','4562','5722','5723','5724','5728','5733','5734','5738','5739','V427',
                        '570','571'],
       'chronic_ulcer':['5317','5319','5327','5329','5337','5339','5347','5349'],
       'hiv_aids':['042','043','044'],
       'lymphoma':['2030','2386', '200','201','202'],
        'metastasis_solid_tumor':['140','141','142','143','144','145','146','147','148','149','150','151','152'
    ,'153','154','155','156','157','158','159','160','161','162','163','164','165'
    ,'166','167','168','169','170','171','172','174','175','176','177','178','179'
    ,'180','181','182','183','184','185','186','187','188','189','190','191','192'
    ,'193','194','195'],
       'rheumatoid_arthiritis':['72889','72930', 
                                '7010','7100','7101','7102','7103','7104','7108','7109','7112','7193','7285',
                                '446','714','720','725'],
       'coagulation_deficiency':['2871','2873','2874','2875',
                                 '286'],
       'obesity':['2780'],
       'weight_loss':['7832','7994',
                      '260','261','262','263'],
       'fluid_electrolyte_disorders':['2536','276'],
       'blood_loss_anemia':['2800'],
       'deficiency_anemia':['2801','2808','2809', '281'],
       'alcohol_abuse':['2652','2911','2912','2913','2915','2918','2919',
                                    '3030','3039','3050','3575','4255','5353','5710','5711','5712','5713','V113',
                                   '980'],
       'drug_abuse':['V6542',
                     '3052','3053','3054','3055','3056','3057','3058','3059',
                     '292','304'],
       'psychoses':['29604','29614','29644','29654','2938','295','297','298'],
       'depression':['2962','2963','2965','3004','309','311']}

mapping_score = pd.DataFrame({'congestive_heart_failure':9,
       'cardiac_arrhythmias':8,
       'valvular_disease':0,
       'pulmonary_circulation_disorder':3,
       'peripheral_vascular_disorder':4,
       'hypertension':-2,
       'paralysis':4,
       'other_neurological':5,
       'chronic_pulmonary_disease':3,
       'diabetes_w_complications':1,
       'hypothyroidism':0,
       'renal_failure':7,
       'liver_disease':7,
       'chronic_ulcer':0,
       'hiv_aids':0,
       'lymphoma':8,
        'metastasis_solid_tumor':17,
       'rheumatoid_arthiritis':0,
       'coagulation_deficiency':12,
       'obesity':-5,
       'weight_loss':10,
       'fluid_electrolyte_disorders':11,
       'blood_loss_anemia':-3,
       'deficiency_anemia':0,
       'alcohol_abuse':0,
       'drug_abuse':-11,
       'psychoses':-6,
       'depression':-5}, index = [0])

def get_mapping(icd_code, mapping):
    for k,v in mapping.items():
        if str(icd_code) in v:
            return k
        elif str(icd_code)[:4] in v:
            return k
        elif str(icd_code)[:3] in v:
            return k
    return None

diagnosis_icd["ICD9_CODE"] = diagnosis_icd.ICD9_CODE.apply(lambda x: get_mapping(x, mapping) if pd.notnull(x) else None)

diagnosis_icd.dropna(subset = ['ICD9_CODE'], axis =0, inplace = True)

diagnosis_icd = diagnosis_icd.drop_duplicates(['SUBJECT_ID', 'HADM_ID','ICD9_CODE'])[['SUBJECT_ID', 'HADM_ID','ICD9_CODE']].pivot_table(index=['SUBJECT_ID', 'HADM_ID'], 
                                   columns='ICD9_CODE',
                                   aggfunc=len, fill_value = 0).reset_index()

diagnosis_icd["ELIXHAUSER_SID30"] = diagnosis_icd.iloc[:,2:].multiply(np.array(mapping_score[list(diagnosis_icd.iloc[:,2:].columns)]), axis='columns').fillna(0).sum(axis = 1)

diagnosis_icd = diagnosis_icd[['SUBJECT_ID', 'HADM_ID','ELIXHAUSER_SID30']]

"""### Merging Individual Data and Filling Missing values"""

import functools
_dfs = [admissions, diagnosis_icd, charts, labs, icustays_final]
train_data = functools.reduce(lambda left,right: pd.merge(left,right,on=["SUBJECT_ID","HADM_ID"], how="inner"), _dfs)

train_data.isnull().sum()

# There are still some missing values in the final dataset, firstly we will try to backfill them at subjectid level, which means the value in next hospital admission will become equal to previous hospital admission
train_data = train_data.sort_values(['SUBJECT_ID','ADMITTIME'])
train_data = train_data.groupby(['SUBJECT_ID']).apply(lambda x: x.bfill())

train_data.isnull().sum()

# null_vals = train_data.isnull().sum().reset_index()
# null_vals.columns = ["column_name","count_of_missing"]

train_data[['mean_Diastolic_BP',
 'mean_Glucose',
 'mean_Heart_Rate',
 'mean_Resp_Rate',
 'mean_Systolic_BP',
 'mean_Temperature',
 'std_Diastolic_BP',
 'std_Glucose',
 'std_Heart_Rate',
 'std_Resp_Rate',
 'std_Systolic_BP',
 'std_Temperature',
 'mean_Albumin',
 'mean_Calcium, Total',
 'mean_Hematocrit',
 'mean_Platelet Count',
 'mean_Potassium',
 'mean_Sodium',
 'std_Albumin',
 'std_Calcium, Total',
 'std_Hematocrit',
 'std_Platelet Count',
 'std_Potassium',
 'std_Sodium',
 'LOS']] = train_data.groupby(['SUBJECT_ID'])['mean_Diastolic_BP',
 'mean_Glucose',
 'mean_Heart_Rate',
 'mean_Resp_Rate',
 'mean_Systolic_BP',
 'mean_Temperature',
 'std_Diastolic_BP',
 'std_Glucose',
 'std_Heart_Rate',
 'std_Resp_Rate',
 'std_Systolic_BP',
 'std_Temperature',
 'mean_Albumin',
 'mean_Calcium, Total',
 'mean_Hematocrit',
 'mean_Platelet Count',
 'mean_Potassium',
 'mean_Sodium',
 'std_Albumin',
 'std_Calcium, Total',
 'std_Hematocrit',
 'std_Platelet Count',
 'std_Potassium',
 'std_Sodium',
 'LOS'].transform(lambda x: x.fillna(x.mean()))

train_data['AGE_BINS'] = pd.cut(x=train_data['AGE'], bins=[18, 29, 39, 49, 59, 69,74,80,85,90,100], labels=['20s', '30s', '40s','50s','60s','75s','80s',
                                                                                   '85s','90s','100s'])

train_data[['ETHNICITY']] = train_data.groupby(['GENDER','AGE_BINS'])['ETHNICITY'].transform(lambda x: x.fillna(x.mode()[0]))

train_data[['mean_Diastolic_BP',
 'mean_Glucose',
 'mean_Heart_Rate',
 'mean_Resp_Rate',
 'mean_Systolic_BP',
 'mean_Temperature',
 'std_Diastolic_BP',
 'std_Glucose',
 'std_Heart_Rate',
 'std_Resp_Rate',
 'std_Systolic_BP',
 'std_Temperature',
 'mean_Albumin',
 'mean_Calcium, Total',
 'mean_Hematocrit',
 'mean_Platelet Count',
 'mean_Potassium',
 'mean_Sodium',
 'std_Albumin',
 'std_Calcium, Total',
 'std_Hematocrit',
 'std_Platelet Count',
 'std_Potassium',
 'std_Sodium',
 'LOS']] = train_data.groupby(['ETHNICITY','GENDER','AGE_BINS'])['mean_Diastolic_BP',
 'mean_Glucose',
 'mean_Heart_Rate',
 'mean_Resp_Rate',
 'mean_Systolic_BP',
 'mean_Temperature',
 'std_Diastolic_BP',
 'std_Glucose',
 'std_Heart_Rate',
 'std_Resp_Rate',
 'std_Systolic_BP',
 'std_Temperature',
 'mean_Albumin',
 'mean_Calcium, Total',
 'mean_Hematocrit',
 'mean_Platelet Count',
 'mean_Potassium',
 'mean_Sodium',
 'std_Albumin',
 'std_Calcium, Total',
 'std_Hematocrit',
 'std_Platelet Count',
 'std_Potassium',
 'std_Sodium',
 'LOS'].transform(lambda x: x.fillna(x.mean()))

train_data.drop(["AGE_BINS", "ADMITTIME"], axis = 1, inplace = True)

train.to_csv("./train.csv", index = None)

visit_plot = train_data.IS_READMISSION.value_counts().plot(kind = 'barh')
visit_plot.set_xlabel('Number of Patient Hospital Visits')
visit_plot.set_ylabel('Hospital Readmission')

"""# Patient Representation"""

import pandas as pd
import numpy as np
import random
import re

data =pd.read_csv("./train.csv", index_col = None)

data.columns = [re.sub(r"[,.;@#?!&$]+\ *", " ",x).replace('/\s\s+/g', ' ').replace(" ","_") for x in data.columns]

num_cols = ['AGE', 'ELIXHAUSER_SID30', 'mean_Diastolic_BP', 'mean_Glucose',
       'mean_Heart_Rate', 'mean_Resp_Rate', 'mean_Systolic_BP',
       'mean_Temperature', 'std_Diastolic_BP', 'std_Glucose', 'std_Heart_Rate',
       'std_Resp_Rate', 'std_Systolic_BP', 'std_Temperature', 'mean_Albumin',
       'mean_Calcium_Total', 'mean_Hematocrit', 'mean_Platelet_Count',
       'mean_Potassium', 'mean_Sodium', 'std_Albumin', 'std_Calcium_Total',
       'std_Hematocrit', 'std_Platelet_Count', 'std_Potassium', 'std_Sodium']

from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
data_minmax = min_max_scaler.fit(data[num_cols])

data_num = data_minmax.transform(data[num_cols])

data_scaled = pd.concat([pd.DataFrame(data_num, columns = num_cols), 
                   data[['INSURANCE', 'ETHNICITY', 'GENDER', 'FIRST_CAREUNIT','IS_READMISSION']]],
                 axis = 1)

from sklearn.model_selection import train_test_split
train, val = train_test_split(data_scaled, test_size=0.2)

import os
import random
import tensorflow as tf
from tensorflow import feature_column
from tensorflow.keras import layers

tf.keras.backend.set_floatx('float32')
tf.random.set_seed(123)
np.random.seed(123)
random.seed(123)
os.environ['PYTHONHASHSEED']=str(123)

def df_to_dataset(dataframe, target_col_name, shuffle=True, batch_size=32, autoenc=True):
    """
    A utility method to create a tf.data dataset from a Pandas Dataframe
    """
    dataframe = dataframe.copy()
    labels = dataframe.pop(target_col_name)
    
    if autoenc:
        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), feature_layer(dict(dataframe)).numpy()))
    else:
        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size)
    return ds

# Note:- Although for encoder target column doesn't make sense, the function is kept generic for later use as well.

# for feature_batch, label_batch in train_ds.take(1):
#     print('Every feature:', list(feature_batch.keys()))
#     print('A batch of ages:', feature_batch['AGE'])
#     print('A batch of targets:', label_batch )

feature_columns = []
# numeric cols
for numeric_cols in num_cols:    
    feature_columns.append(feature_column.numeric_column(numeric_cols))

# categorical cols
for cat_cols in ['INSURANCE', 'ETHNICITY', 'GENDER', 'FIRST_CAREUNIT']:
    categorical_column = feature_column.categorical_column_with_vocabulary_list(
      cat_cols, train[cat_cols].unique())
    indicator_column = feature_column.indicator_column(categorical_column)
    feature_columns.append(indicator_column)

# After defining our feature layer we will create a layer to input them to our keras model. We will DenseFeatures layer for this
feature_layer = layers.DenseFeatures(feature_columns)

batch_size = 32 
train_ds = df_to_dataset(train, 
                         target_col_name='IS_READMISSION', batch_size=batch_size)
val_ds = df_to_dataset(val, 
                         target_col_name='IS_READMISSION', batch_size=batch_size)
full_ds = df_to_dataset(data_scaled, 
                         target_col_name='IS_READMISSION', 
                        batch_size=batch_size,
                       shuffle = False)

# To modularize the shape of output layer in the autoencoder
output_shape = feature_layer(next(iter(train_ds))[0]).numpy().shape[1]

encoder = tf.keras.Sequential([
    feature_layer,
    layers.Dense(32, activation = "selu", kernel_initializer="lecun_normal"),
    layers.Dense(16, activation = "selu", kernel_initializer="lecun_normal"),
    layers.Dense(8, activation = "selu", kernel_initializer="lecun_normal"),
    layers.Dense(4, activation = "selu", kernel_initializer="lecun_normal"),
    layers.Dense(2, activation = "selu", kernel_initializer="lecun_normal")
])

decoder = tf.keras.Sequential([
    layers.Dense(4, activation = "selu", kernel_initializer="lecun_normal", input_shape=[2]),
    layers.Dense(8, activation = "selu",kernel_initializer="lecun_normal"),
    layers.Dense(16, activation = "selu",kernel_initializer="lecun_normal"),
    layers.Dense(32, activation = "selu",kernel_initializer="lecun_normal"),
    layers.Dense(output_shape, activation = "selu", kernel_initializer="lecun_normal"),
])

stacked_ae = tf.keras.Sequential([encoder, decoder])
stacked_ae.compile(loss='mse', metrics = "mean_absolute_error",
                   optimizer= tf.keras.optimizers.Adam(learning_rate=0.01))

history = stacked_ae.fit(train_ds,
                         validation_data = val_ds,
                         epochs=15)

# Plotting libraries and parameters
import matplotlib.pyplot as plt
plt.figure(figsize=(12,8))
import seaborn as sns

mae = history.history['mean_absolute_error']
val_mae = history.history['val_mean_absolute_error']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(15)

plt.subplot(1, 2, 1)
plt.plot(epochs_range, mae, label='Training MAE')
plt.plot(epochs_range, val_mae, label='Validation MAE')
plt.legend(loc='upper right')
plt.title('Training and Validation MAE')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

stacked_ae.save('trained_model')

"""# Cohort Discovery"""

codings = encoder.predict(full_ds)

k_means_data = pd.concat([data[["SUBJECT_ID","IS_READMISSION"]],
                          pd.DataFrame(codings, columns = ["val1","val2"])],
                         axis = 1)

# ! pip install tabulate

from sklearn.cluster import KMeans
# Randomly choosing some value to begin experiments
k = 4
kmeans = KMeans(n_clusters=k, random_state=123)
cluster_predictions = kmeans.fit_predict(codings)

def plot_data(X, labels = k_means_data["IS_READMISSION"]):
    plt.scatter(X[:, 0], X[:, 1], c = labels, cmap='inferno')

def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='o', s=30, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=50, linewidths=50,
                color=cross_color, zorder=11, alpha=1)

def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)

plt.figure(figsize=(8, 4))
plot_decision_boundaries(kmeans, codings)
plt.show()

kmeans_iter1 = KMeans(n_clusters=4, init="k-means++", n_init=5,
                     max_iter=1, random_state=123)
kmeans_iter2 = KMeans(n_clusters=4, init="k-means++", n_init=5,
                     max_iter=2, random_state=123)
kmeans_iter3 = KMeans(n_clusters=4, init="k-means++", n_init=5,
                     max_iter=3, random_state=123)
kmeans_iter1.fit(codings)
kmeans_iter2.fit(codings)
kmeans_iter3.fit(codings)

"""#### Note the plotting code below is adapated from https://github.com/ageron/handson-ml2 """

plt.figure(figsize=(10, 8))

plt.subplot(321)
plot_data(codings)
plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')
plt.ylabel("$x_2$", fontsize=14, rotation=0)
plt.tick_params(labelbottom=False)
plt.title("Update the centroids (initially randomly)", fontsize=14)

plt.subplot(322)
plot_decision_boundaries(kmeans_iter1, codings, show_xlabels=False, show_ylabels=False)
plt.title("Label the instances", fontsize=14)

plt.subplot(323)
plot_decision_boundaries(kmeans_iter1, codings, show_centroids=False, show_xlabels=False)
plot_centroids(kmeans_iter2.cluster_centers_)

plt.subplot(324)
plot_decision_boundaries(kmeans_iter2, codings, show_xlabels=False, show_ylabels=False)

plt.subplot(325)
plot_decision_boundaries(kmeans_iter2, codings, show_centroids=False)
plot_centroids(kmeans_iter3.cluster_centers_)

plt.subplot(326)
plot_decision_boundaries(kmeans_iter3, codings, show_ylabels=False)

plt.show()

kmeans__ncluster = [KMeans(n_clusters=x, init="k-means++",
                           max_iter = 3,
                           n_init = 5,
                           random_state=123).fit(codings)
                for x in range(1, 10)]
inertias = [kmeans_model.inertia_ for kmeans_model in kmeans__ncluster]

from sklearn.metrics import silhouette_score
silhouette_scores = [silhouette_score(codings, kmeans_model.labels_)
                     for kmeans_model in kmeans__ncluster[1:]]

plt.figure(figsize=(12, 6))

plt.subplot(121)
plt.plot(range(1, 10), inertias, "ro-")
plt.xlabel("Number of Clusters", fontsize=15)
plt.ylabel("Inertia", fontsize=15)

plt.subplot(122)
plt.plot(range(2, 10), silhouette_scores, "ro-")
plt.xlabel("Number of Clusters", fontsize=15)
plt.ylabel("Silhouette score", fontsize=15)
plt.show()

from sklearn.cluster import KMeans
# Randomly choosing some value to begin experiments
k = 4
kmeans = KMeans(n_clusters=k, init="k-means++", n_init=5, max_iter = 3, random_state=123)
cluster_predictions = kmeans.fit_predict(codings)

k_means_data["cluster_label"] = cluster_predictions

count_labels = k_means_data.groupby(['cluster_label','IS_READMISSION'])['SUBJECT_ID'].count().reset_index()
sample_count = pd.pivot_table(count_labels, index="cluster_label", columns=['IS_READMISSION'], values="SUBJECT_ID").reset_index()
sample_count.columns = [sample_count.columns.name + "_" +str(x) if type(x)!=str else x for x in list(sample_count.columns)] 
sample_count.reset_index(drop = True, inplace = True)
sample_count["Total_Samples"] = sample_count[["IS_READMISSION_0","IS_READMISSION_1"]].apply(sum, axis =1)
sample_count["Readmission_Percentage"] = (sample_count["IS_READMISSION_1"]/sample_count["Total_Samples"])*100

print(sample_count.to_markdown())

k_means_data.to_csv("cluster_label.csv")

"""# Multi Task Learning"""

import pandas as pd
import numpy as np

# data =pd.read_csv("./train.csv", index_col = None)

# data["cluster_labels"] = cluster_predictions

# data.columns = [re.sub(r"[,.;@#?!&$]+\ *", " ",x).replace('/\s\s+/g', ' ').replace(" ","_") for x in data.columns]

data = pd.read_csv("./train_w_labels.csv", index_col = None)

# Updating the num_cols and categorical_cols
num_cols = ['AGE', 'DISCHARGE_DURATION', 'ELIXHAUSER_SID30', 'mean_Diastolic_BP', 'mean_Glucose',
       'mean_Heart_Rate', 'mean_Resp_Rate', 'mean_Systolic_BP',
       'mean_Temperature', 'std_Diastolic_BP', 'std_Glucose', 'std_Heart_Rate',
       'std_Resp_Rate', 'std_Systolic_BP', 'std_Temperature', 'mean_Albumin',
       'mean_Calcium_Total', 'mean_Hematocrit', 'mean_Platelet_Count',
       'mean_Potassium', 'mean_Sodium', 'std_Albumin', 'std_Calcium_Total',
       'std_Hematocrit', 'std_Platelet_Count', 'std_Potassium', 'std_Sodium','LOS','TOTAL_TRANSFERS']
target_col = ['IS_READMISSION']
categorical_col = ['ADMISSION_TYPE','DISCHARGE_LOCATION','INSURANCE', 'ETHNICITY', 'GENDER', 'FIRST_CAREUNIT']

# Updating Scaling with new numerical columns
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
data_minmax = min_max_scaler.fit(data[num_cols])

data_num = data_minmax.transform(data[num_cols])

data_scaled = pd.concat([pd.DataFrame(data_num, columns = num_cols), 
                   data[categorical_col + target_col + ["cluster_labels"]]],
                 axis = 1)

from sklearn.model_selection import train_test_split
train, val = train_test_split(data_scaled, test_size=0.2)

def gen_labels(readm_val, cluster_val):
    """
    Helper function to generate labels for multi-output system
    """
    res = [0,0,0,0]
    if readm_val:
        res[cluster_val] = 1
    return res

# [[x[0] for x in labels], [x[1] for x in labels], [x[2] for x in labels],[x[3] for x in labels]]

# ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), [x[0] for x in labels], 
#                                             [x[1] for x in labels], [x[2] for x in labels],
#                                             [x[3] for x in labels]))

# ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), {'cluster_0':[x[0] for x in labels],
#                                                               'cluster_1':[x[1] for x in labels],
#                                                               'cluster_2':[x[2] for x in labels],
#                                                               'cluster_3':[x[3] for x in labels]}))

import os
import tensorflow as tf
from tensorflow import feature_column
from tensorflow.keras import layers

tf.keras.backend.set_floatx('float32')

feature_columns = []
feature_layer_inputs = {}
# numeric cols
for numeric_cols in num_cols:    
    feature_columns.append(feature_column.numeric_column(numeric_cols))
    feature_layer_inputs[numeric_cols] = tf.keras.Input(shape=(1,), name=numeric_cols)

# categorical cols
for cat_cols in categorical_col:
    categorical_column = feature_column.categorical_column_with_vocabulary_list(
      cat_cols, train[cat_cols].unique())
    indicator_column = feature_column.indicator_column(categorical_column)
    feature_columns.append(indicator_column)
    feature_layer_inputs[cat_cols] = tf.keras.Input(shape=(1,), name=cat_cols, dtype=tf.string)

# After defining our feature layer we will create a layer to input them to our keras model. We will DenseFeatures layer for this
feature_layer = layers.DenseFeatures(feature_columns)

def df_to_dataset_multio(dataframe, target_col_name = 'IS_READMISSION'):
    """
    A utility method to create a Input data for the MTL NN
    """
    dataframe = dataframe.copy()
    labels = [gen_labels(row[1], row[2]) for row in dataframe[[target_col_name, 'cluster_labels']].itertuples()]

    assert np.sum(labels) == dataframe[target_col_name].sum()
    dataframe.drop([target_col_name, 'cluster_labels'], axis = 1, inplace = True)

    # Generating Tensorflow Dataset
    train_ds = feature_layer(dict(dataframe)).numpy()
    y_train_ds = {'cluster_0':np.array([x[0] for x in labels]),
                  'cluster_1':np.array([x[1] for x in labels]),
                  'cluster_2':np.array([x[2] for x in labels]),
                  'cluster_3':np.array([x[3] for x in labels])}
    return train_ds, y_train_ds

train_ds, train_col_map = df_to_dataset_multio(train)
val_ds, val_col_map = df_to_dataset_multio(val)

def get_data_generator(df, cluster_map, batch_size=32):
    """
    Generator function which yields the input data and output for different clusters
    """
    feats, cluster_0, cluster_1, cluster_2, cluster_3 = [], [], [], [], []
    while True:
        for i in range(len(df)):
            feats.append(df[i])
            cluster_0.append(cluster_map['cluster_0'][i])
            cluster_1.append(cluster_map['cluster_1'][i])
            cluster_2.append(cluster_map['cluster_2'][i])
            cluster_3.append(cluster_map['cluster_3'][i])
            if len(feats) >= batch_size:
                yield np.array(feats), [np.array(cluster_0), np.array(cluster_1), np.array(cluster_2), np.array(cluster_3)]
                feats, cluster_0, cluster_1, cluster_2, cluster_3 = [], [], [], [], []

input_layer = layers.Input(shape = (train_ds.shape[1]))
_ = layers.Dense(32, activation = "selu", kernel_initializer="lecun_normal")(input_layer)
_ = layers.Dense(16, activation = "selu", kernel_initializer="lecun_normal")(_)
last_shared_layer = layers.Dense(8, activation = "selu", kernel_initializer="lecun_normal")(_)

_ = layers.Dense(4, activation = "selu", kernel_initializer="lecun_normal")(last_shared_layer)
cluster_0 = layers.Dense(1, activation = "sigmoid", name='cluster_0')(_)

_ = layers.Dense(4, activation = "selu", kernel_initializer="lecun_normal")(last_shared_layer)
cluster_1 = layers.Dense(1, activation = "sigmoid", name='cluster_1')(_)

_ = layers.Dense(4, activation = "selu", kernel_initializer="lecun_normal")(last_shared_layer)
cluster_2 = layers.Dense(1, activation = "sigmoid", name='cluster_2')(_)

_ = layers.Dense(4, activation = "selu", kernel_initializer="lecun_normal")(last_shared_layer)
cluster_3 = layers.Dense(1, activation = "sigmoid", name='cluster_3')(_)


mtl_model = tf.keras.Model(inputs = input_layer, 
                           outputs = [cluster_0, cluster_1, cluster_2, cluster_3])

mtl_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), 
              loss={'cluster_0': 'binary_crossentropy', 
                    'cluster_1': 'binary_crossentropy', 
                    'cluster_2': 'binary_crossentropy',
                    'cluster_3': 'binary_crossentropy'},
              loss_weights={'cluster_0': 0.25, 
                    'cluster_1': 0.25, 
                    'cluster_2': 0.25,
                    'cluster_3': 0.25},
              metrics={'cluster_0': 'AUC', 
                    'cluster_1': 'AUC', 
                    'cluster_2': 'AUC',
                    'cluster_3': 'AUC'})

batch_size = 32
valid_batch_size = 32
train_gen = get_data_generator(train_ds, train_col_map,  batch_size=batch_size)
valid_gen = get_data_generator(val_ds, val_col_map, batch_size=valid_batch_size)

history = mtl_model.fit_generator(train_gen,
                    steps_per_epoch=len(train)//batch_size,
                    epochs=10,
                    validation_data=valid_gen,
                    validation_steps=len(val)//valid_batch_size)

from tensorflow.keras.layers import Layer
mtl_model._layers = [
    layer for layer in mtl_model._layers if isinstance(layer, Layer)
]

tf.keras.utils.plot_model(mtl_model)

# Plotting libraries and parameters
import matplotlib.pyplot as plt
plt.figure(figsize=(20,12))
import seaborn as sns

epochs_range = range(10)

plt.subplot(2, 4, 1)
plt.plot(epochs_range, history.history['cluster_0_auc'], label='Training AUC')
plt.plot(epochs_range, history.history['val_cluster_0_auc'], label='Validation AUC')
# plt.legend(loc='upper right')
plt.title('Cluster 0')

plt.subplot(2, 4, 2)
plt.plot(epochs_range, history.history['cluster_1_auc_1'], label='Training AUC')
plt.plot(epochs_range, history.history['val_cluster_1_auc_1'], label='Validation AUC')
# plt.legend(loc='upper right')
plt.title('Cluster 1')

plt.subplot(2, 4, 3)
plt.plot(epochs_range, history.history['cluster_2_auc_2'], label='Training AUC')
plt.plot(epochs_range, history.history['val_cluster_2_auc_2'], label='Validation AUC')
# plt.legend(loc='upper right')
plt.title('Cluster 2')

plt.subplot(2, 4, 4)
plt.plot(epochs_range, history.history['cluster_3_auc_3'], label='Training AUC')
plt.plot(epochs_range, history.history['val_cluster_3_auc_3'], label='Validation AUC')
plt.legend(loc='upper right')
plt.title('Cluster 3')

plt.subplot(2, 4, 5)
plt.plot(epochs_range, history.history['cluster_0_loss'], label='Cluster 0 :- Loss')
plt.plot(epochs_range, history.history['val_cluster_0_loss'], label='Cluster 0:- Val Loss')
# plt.legend(loc='upper right')
plt.title('Cluster 0')

plt.subplot(2, 4, 6)
plt.plot(epochs_range, history.history['cluster_1_loss'], label='Cluster 1 :- Loss')
plt.plot(epochs_range, history.history['val_cluster_1_loss'], label='Cluster 1:- Val Loss')
# plt.legend(loc='upper right')
plt.title('Cluster 1')

plt.subplot(2, 4, 7)
plt.plot(epochs_range, history.history['cluster_2_loss'], label='Cluster 2 :- Loss')
plt.plot(epochs_range, history.history['val_cluster_2_loss'], label='Cluster 2:- Val Loss')
# plt.legend(loc='upper right')
plt.title('Cluster 2')

plt.subplot(2, 4, 8)
plt.plot(epochs_range, history.history['cluster_3_loss'], label='Loss')
plt.plot(epochs_range, history.history['val_cluster_3_loss'], label='Val Loss')
plt.legend(loc='upper right')
plt.title('Cluster 3')
plt.show()

mtl_model.save('trained_model_mtl')

