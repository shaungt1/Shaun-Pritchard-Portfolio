{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of LogisticRegresion10.ipynb","private_outputs":true,"provenance":[{"file_id":"1DvdVkwFWDk1CjhxuyXrblPOyf9TIsYvn","timestamp":1637187750630}],"authorship_tag":"ABX9TyMWmuI8oJVJyRXtRSCwZwRI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pfJYEoZT0uZG"},"source":["# **Assignment 3  (From Scratch)**"]},{"cell_type":"markdown","metadata":{"id":"6MDdIQRsINp7"},"source":["## **Penalized Logistic Regression**"]},{"cell_type":"markdown","metadata":{"id":"3Ly0Ym7-0vxC"},"source":["- **Programmers:**\n","  - Shaun Pritchard\n","  - Ismael A Lopez\n","- **Date:** 11-15-2021\n","- **Assignment:** 2\n","- **Prof:** M.DeGiorgio\n","\n","<hr>\n","\n","### **Overview: Assignment 3**\n","\n","- In this assignment you will still be analyzing human genetic data from ùëÅ = 183 training\n","observations (individuals) sampled across the world. The goal is to fit a model that can predict\n","(classify) an individual‚Äôs ancestry from their genetic data that has been projected along ùëù = 10\n","top principal components (proportion of variance explained is 0.2416) that we use as features\n","rather than the raw genetic data\n","\n","- Using ridge regression, fit a penalized (regularized) logistic (multinomial) regression with model parameters obtained by batch gradient descent. Based on K = 5 continental ancestries (African, European, East Asian, Oceanian, or Native American), predictions will be made. Ridge regression will permit parameter shrinkage (tuning parameter ùúÜ ‚â• 0) to mitigate overfitting. In order to infer the bestfit model parameters on the training dataset, the tuning parameter will be selected using five-fold cross validation. After training, the model will be used to predict new test data points.\n","\n"]},{"cell_type":"code","metadata":{"id":"zrea_glVsGcq"},"source":["\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKIpf8zQsOvf"},"source":["train_df = pd.read_csv('TrainingData_N183_p10.csv')\n","test_df = pd.read_csv('TestData_N111_p10.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkRiPmYTsOyV"},"source":["train_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhaMl5Jww--x"},"source":["test_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2BsxTwGww_By"},"source":["# recode the categories\n","class_names = train_df['Ancestry'].unique().tolist()\n","n_classes = len(class_names)\n","train_df['AncestryRecoded'] = train_df['Ancestry'].apply(lambda x: class_names.index(x))\n","train_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1iZqPZRJslzX"},"source":["train_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5oyPtPcCJYH"},"source":["# Seperate dependant categorical feature data\n","Y_train_names = train_df['Ancestry'].tolist()\n","Y_test_names = test_df['Ancestry'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2mBPvRSsl2T"},"source":["# Separate training feature predictors from responses\n","X_train = np.float32(train_df.to_numpy()[:, :-2])\n","Y_train = train_df['AncestryRecoded'].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggVmpaFeCNj_"},"source":["# Separate test feature predictors from responses\n","X_test = np.float32(test_df.to_numpy()[:, :-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6ROxL2rsl5k"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gptDjmHtMZLF"},"source":["## **Set Global Vairibles**"]},{"cell_type":"code","metadata":{"id":"laoOpyhxMZYI"},"source":["# Set local variables\n","# 9-Tuning Parms\n","# Œª  =  10 ** np.arange(-4., 4.)\n","lambdas =  10 ** np.arange(-4., 4.)\n","\n","# 6 learning & convergence rate\n","# Œ± =  1e-4\n","alpha =  1e-4\n","\n","# K-folds\n","nfolds = 5\n","\n","# itterations\n","n_iters = 10000 \n","\n","#log base of lambda\n","# Œª_log = np.log10(Œª) \n","\n","# Set verbose to True\n","verbose = True\n","\n","# Set n x m matrix variable\n","preds = X_train\n","\n","# Set n vector variable\n","responses  = Y_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RP7PyeRRAFu4"},"source":["## **Instantiate Data**"]},{"cell_type":"code","metadata":{"id":"k6LrrIsMsl8m"},"source":["# encode Y response in CV\n","def _make_one_hot_responses(response_vector, n_classes):\n","   response_vector = np.int64(response_vector)\n","   n_samples = response_vector.shape[0]\n","   response_mat = np.zeros([n_samples, n_classes])\n","   response_mat[np.arange(n_samples), response_vector] = 1\n","   return response_mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blXxLauwCokE"},"source":["def _shuffle_data(preds, responses):\n","    data = np.concatenate((preds, responses[:, None]), 1)\n","    np.random.shuffle(data)\n","    return data[:, :-1], data[:, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgxuVTzcAE1Y"},"source":["# shuffle the data\n","x, y = _shuffle_data(preds, responses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deEiexERAE4g"},"source":["# get number of samples and number of features\n","n_samples = x.shape[0]\n","n_preds = x.shape[1]     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REwNEn8UAE7a"},"source":["# get number of training classes \n","n_classes = np.unique(y).size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_xKJF_b3AE-S"},"source":[" # make one-hot response mat\n"," y = _make_one_hot_responses(y, n_classes)      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"de3RUM4-AFBL"},"source":["# matrix to store the cross validation results \n","cv_vals = np.zeros([nfolds, len(lambdas)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMzRGRfPBchL"},"source":["# determine the number of validation samples and their inds based on nfolds \n","n_val_samples = int(np.ceil(n_samples / nfolds)) \n","val_inds = list(range(0, n_samples, n_val_samples))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rI3jlasqBckA"},"source":["# create a tensor to store the trained coefficient vectors\n","B_trained = np.zeros([nfolds, len(lambdas), n_preds + 1, n_classes])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etHhdrBQBcmY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9j4wHxaB0kM"},"source":["## **Implemnt functions**"]},{"cell_type":"code","metadata":{"id":"l_9UwIrKAFEA"},"source":["# Standardize X\n","def _standardize(x, mean_vec, std_vec):\n","   return (x - mean_vec) / std_vec "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhLPoYjfC6xY"},"source":["# def _initialize_B():\n","#     return np.zeros([n_preds + 1, n_classes])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9GcygS6C60O"},"source":["def predict(x):\n","    # assert(mean_vec is not None and std_vec is not None), \\\n","    # 'Model must be trained before predicting.'\n","  x = _standardize(x, mean_vec, std_vec)\n","  x = _add_intercept(x)\n","  preds = np.exp(np.matmul(x, B))\n","  return preds / np.sum(preds, 1)[:, None]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMSn9nz9D-bG"},"source":["def _get_folds(val_ind):\n","    if val_ind + n_val_samples <= n_samples:\n","        val_inds = np.arange(val_ind, val_ind + n_val_samples)\n","    else:\n","        val_inds = np.arange(val_ind, n_samples)\n","            \n","    x_val = x[val_inds]\n","    x_train = np.delete(x, val_inds, axis = 0)\n","    y_val = y[val_inds]\n","    y_train = np.delete(y, val_inds, axis = 0)\n","    return x_train, x_val, y_train, y_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hvp3-5JGD-hG"},"source":["def score(x, y, B):\n","    unnorm_prob_mat = np.exp(np.matmul(x, B))\n","    norm_prob_mat = unnorm_prob_mat / np.sum(unnorm_prob_mat, 1)[:, None]\n","    ce = -(1 / x.shape[0]) * np.sum(np.sum(y * np.log10(norm_prob_mat), 1))\n","    return ce"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wY2E9eu9EoiJ"},"source":["def _add_intercept(x):\n","    intercept_col = np.ones([x.shape[0], 1])\n","    return np.concatenate((intercept_col, x), 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7qfhmq9D-d_"},"source":["def LogisticRegresion(x, y, B, lambda_):\n","\n","    unnorm_prob_mat = np.exp(np.dot(x, B))\n","\n","    norm_prob_mat = unnorm_prob_mat / np.sum(unnorm_prob_mat, 1)[:, None]\n","\n","    intercept_mat = B.copy()\n","\n","    intercept_mat[1:] = 0\n","    \n","    B = B + alpha * (np.matmul(np.transpose(x), y - norm_prob_mat) - 2 * lambda_ * (B - intercept_mat))\n","    return B"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gjn-W_A0EyeW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-dBXJCDEzHw"},"source":["## **Penlized Ridge CV**"]},{"cell_type":"code","metadata":{"id":"YvHT3sDUsO05"},"source":["# Original function\n","for i_lambda, lambda_ in enumerate(lambdas):\n","    for i_fold, val_ind in zip(range(nfolds), val_inds):\n","        # get the folds\n","        x_train, x_val, y_train, y_val = _get_folds(val_ind)\n","\n","\n","        # standardize x \n","        mean_vec, std_vec = np.mean(x_train, 0), np.std(x_train, 0)\n","        x_train = _standardize(x_train, mean_vec, std_vec)\n","        x_val = _standardize(x_val, mean_vec, std_vec)\n","                \n","        # add intercept column to design matrix\n","        x_train = _add_intercept(x_train)\n","        x_val = _add_intercept(x_val)\n","\n","\n","        # initialize Beta for this lambda and fold\n","        B =  np.zeros([n_preds + 1, n_classes])\n","\n","        for iter in range(n_iters):\n","            B = LogisticRegresion(x_train, y_train, B, lambda_)\n","\n","        # score this model and store the value \n","        cv_vals[i_fold, i_lambda] = score(x_val, y_val, B)\n","                \n","        # save this coefficient vector\n","        B_trained[i_fold, i_lambda] = B\n","\n","    # # find the best lambda and retrain model\n","    # best_lambda = lambdas[np.argmin(np.mean(cv_vals, 0))]\n","    # mean_vec, std_vec = np.mean(x, 0), np.std(x, 0)\n","    # x = _standardize(x, mean_vec, std_vec)\n","    # x = _add_intercept(x)\n","    # y = y.copy()\n","    # B = np.zeros([n_preds + 1, n_classes])\n","        \n","    # for iter in range(n_iters):\n","    #     B = LogisticRegresion(x, y, B, best_lambda)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qnw_KfqRINkt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vMjjWOU-INoy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r65NBhz9szIB"},"source":["mean_betas = np.mean(logistic_ridge.B_trained, 0)\n","\n","for class_ind, class_name in enumerate(class_names):\n","    mean_beta_k = mean_betas[..., class_ind]\n","\n","    for pred_num in range(1, 1 + logistic_ridge.n_preds):\n","        plt.plot(\n","            logistic_ridge.lambdas, \n","            mean_beta_k[:, pred_num],\n","            label = 'PC{}'.format(pred_num)\n","        )\n","    \n","    plt.xscale('log')\n","    plt.legend(bbox_to_anchor = (1.05, 1), loc = 'upper left')\n","    plt.xlabel('Log Lambda')\n","    plt.ylabel('Coefficient Value')\n","    plt.title(class_name)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PwRPGwAszL8"},"source":["se = np.std(logistic_ridge.cv_vals, 0) / np.sqrt(logistic_ridge.cv_vals.shape[0])\n","plt.errorbar(\n","    logistic_ridge.lambdas, \n","    np.mean(logistic_ridge.cv_vals, 0),\n","    yerr = se\n",")\n","plt.xscale('log')\n","plt.yscale('log')\n","plt.xlabel('Log Lambda')\n","plt.ylabel('Log CV MSE +/- 1 SE')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yf3zot5szNk"},"source":["print('Optimal lambda value: {}'.format(logistic_ridge.best_lambda))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r102hDRlszQN"},"source":["Y_train_hat = logistic_ridge.predict(X_train)\n","Y_train_class = np.argmax(Y_train_hat, 1)\n","print('Training Accuracy: {}'.format(np.mean(Y_train_class == Y_train)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HclnGqLLtSIv"},"source":["\n","Y_test_hat = logistic_ridge.predict(X_test)\n","Y_test_class = np.argmax(Y_test_hat, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p91bUzNGtSLo"},"source":["\n","# add these predictions to the test dataframe\n","new_df_col_names = ['{}Prob'.format(class_name) for class_name in class_names] + ['ClassPredInd']\n","prob_and_ind_array = np.concatenate((Y_test_hat, Y_test_class[:, None]), 1)\n","new_df = pd.DataFrame(prob_and_ind_array, columns = new_df_col_names)\n","test_anc_with_preds = pd.concat([test_df['Ancestry'], new_df], axis = 1)\n","test_anc_with_preds['ClassPredName'] = test_anc_with_preds['ClassPredInd'].apply(lambda x: class_names[int(x)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NW3kaEadtSOq"},"source":["print(test_anc_with_preds.to_string())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WGlG4gPAtSRS"},"source":["test_true_with_probs = test_anc_with_preds.loc[:, 'Ancestry':'NativeAmericanProb']\n","test_true_with_probs_long = pd.melt(\n","    test_true_with_probs,\n","    id_vars = ['Ancestry'],\n","    var_name = 'AncestryPred',\n","    value_name = 'Probability'\n",")\n","test_true_with_probs_long['AncestryPred'] = test_true_with_probs_long['AncestryPred'].apply(lambda x: x.split('Prob')[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2h6fTkjutfeB"},"source":["seaborn.catplot(\n","    data = test_true_with_probs_long[test_true_with_probs_long['Ancestry'] != 'Unknown'],\n","    kind = 'bar',\n","    x = 'Ancestry',\n","    y = 'Probability',\n","    hue = 'AncestryPred',\n","    ci = \"sd\"\n",")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9G-RTNJOtfg7"},"source":[""],"execution_count":null,"outputs":[]}]}